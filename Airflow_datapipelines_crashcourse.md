Data pipelines are the foundation for success in data analytics and
machine learning. Moving data from numerous, diverse sources and
processing it to provide context is the difference between having
data and getting value from it.
The emergence of cloud infrastructure, and cloud data warehouses in particular, has created
an opportunity to rethink the way data pipelines are designed and implemented.

##### What Are Data Pipelines?
Data pipelines are sets of processes that move and transform data
from various sources to a destination where new value can be derived. They are the foundation of analytics, reporting, and machine
learning capabilities.
The complexity of a data pipeline depends on the size, state, and structure of the source data as well as the needs of the analytics project.


#### Common Ingestion Interface and Data Structure
Some of the most common include the following:
- A database behind an application, such as a Postgres or MySQL database
- A layer of abstraction on top of a system such as a REST API
- A stream processing platform such as Apache Kafka
- A shared network file system or cloud storage bucket containing logs, comma-separated value (CSV) files, and other flat files
- A data warehouse or data lake
- Data in HDFS or HBase database





##### Sources
1. Data Pipelines Pocket Reference Book by James Densmore