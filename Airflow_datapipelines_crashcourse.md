Data pipelines are the foundation for success in data analytics and
machine learning. Moving data from numerous, diverse sources and
processing it to provide context is the difference between having
data and getting value from it.
The emergence of cloud infrastructure, and cloud data warehouses in particular, has created
an opportunity to rethink the way data pipelines are designed and implemented.

##### What Are Data Pipelines?
Data pipelines are sets of processes that move and transform data
from various sources to a destination where new value can be derived. They are the foundation of analytics, reporting, and machine
learning capabilities.
The complexity of a data pipeline depends on the size, state, and structure of the source data as well as the needs of the analytics project.


#### Common Ingestion Interface and Data Structure
Some of the most common include the following:
- A database behind an application, such as a Postgres or MySQL database
- A layer of abstraction on top of a system such as a REST API
- A stream processing platform such as Apache Kafka
- A shared network file system or cloud storage bucket containing logs, comma-separated value (CSV) files, and other flat files
- A data warehouse or data lake
- Data in HDFS or HBase database

In addition to the interface, the structure of the data will vary. Here are some common examples:
- JSON from a REST API
- -Well-structured data from a MySQL database
- JSON within columns of a MySQL database table
- Semistructured log data
- CSV, fixed-width format (FWF), and other flat file formats
- JSON in flat files
- Stream output from Kafka


##### A data warehouse 
is a database where data from different systems is stored and modeled to support analysis and other activities related
to answering questions with it. Data in a data warehouse is structured and optimized for reporting and analysis queries.

###### A data lake 
is where data is stored, but without the structure or query optimization of a data warehouse. It will likely contain a high volume
of data as well as a variety of data types. For example, 
- a single data lake  might contain a collection of blog posts stored as text files, 
- flat file extracts from a relational database
- JSON objects containing events generated by sensors in an industrial system. 
It can even store structured data like a standard database, though it’s not optimized for querying such data in the interest of reporting and
analysis.


##### Data transformation
Transforming data is a broad term that is signified by the T in an ETL or ELT process. A transformation can be something as
simple as converting a timestamp stored in a table from one time zone to another. It can also be a more complex operation that
creates a new metric from multiple source columns that are aggregated and filtered through some business logic.
For example, for the sake of protecting personally identifiable information (PII) it may be desirable to turn an email address into a hashed value that is stored in the final destination

##### Data modeling
Data modeling is a more specific type of data transformation. A data model structures and defines data in a format that is
understood and optimized for data analysis. A data model is usually represented as one or more tables in a data warehouse.


##### Workflow Orchestration Platforms
As the complexity and number of data pipelines in an organization grows, it’s important to introduce a workflow orchestration platform to
your data infrastructure. These platforms manage the scheduling  and flow of tasks in a pipeline. 
There are numerous workflow orchestration platforms available such as Apache Airflow, Luigi, and AWS Glue, are designed for more general use cases and are thus used for a wide variety of data pipelines. 



##### Directed Acyclic Graphs
Nearly all modern orchestration frameworks represent the flow and dependencies of tasks in a pipeline as a graph. However, pipeline
graphs have some specific constraints.
Pipeline steps are always directed, meaning they start with a general task or multiple tasks and end with a specific task or tasks. This is
required to guarantee a path of execution. In other words, it ensures that tasks do not run before all their dependent tasks are completed
successfully.
- Pipeline graphs must also be acyclic, meaning that a task cannot point back to a previously completed task. In other words, it cannot
cycle back. If it could, then a pipeline could run endlessly!
With these two constraints in mind, orchestration pipelines produce graphs called directed acyclic graphs (DaGs). 


For example, consider a data pipeline with three tasks. 
- Task A ,the first executes a SQL script that queries data from a relational database and stores the result in a CSV file.
- Task B ,The second runs a Python script that loads the CSV file, cleans, and then reshapes the data before saving a new version of the file.
- Task C , finally, a third task, which runs the COPY command in SQL, loads the CSV created by the second task into a Snowflake data
warehouse.

For example, Task A must complete before Tasks B can start. Once they are both completed, then Task C can
start. Once Task C is complete, the pipeline is completed as well.

## Common Data Pipeline Patterns
Pipelines are built with different goals and constraints. Such as
- Must the data be processed in near real time? 
- Can it be updated daily? 
- Will it be modeled for use in a dashboard or as input to a machine learning model?

#### ETL and ELT
Both patterns are approaches to data processing used to feed data
into a data warehouse and make it useful to analysts and reporting
tools. 
The difference between the two is the order of their final two steps (transform and load), but the design implications in choosing
between them are substantial.

- The extract step gathers data from various sources in preparation for loading and transforming. 
- The load step brings either the raw data (in the case of ELT) or the fully transformed data (in the case of ETL) into the final destination.
Either way, the end result is loading data into the data warehouse, data lake, or other destination.
- The transform step is where the raw data from each source system is combined and formatted in a such a way that it’s useful to
analysts, visualization tools, or whatever use case your pipeline is serving. 
The combination of the extraction and loading steps is often referred to as data ingestion. 

#### EtLT Subpattern
Some examples of the type of transformation that fits into the EtLT subpattern include the following:
At times the steps are required as early in a pipeline as possible for legal or security reasons.
- Deduplicate records in a table
- Parse URL parameters into individual components
- Mask or otherwise obfuscate sensitive data

- With ELT, data engineers can focus on the extract and load
steps in a pipeline (data ingestion), while analysts can utilize SQL to
transform the data that’s been ingested as needed for reporting and
analysis. 
- ELT allows data team members to focus on
their strengths with less interdependencies and coordination.
In addition, the ELT pattern reduces the need to predict exactly what
analysts will do with the data at the time of building extract and load
processes. 

Pipelines built for ML follow the to ELT pattern. The difference is that instead of the
transform step focusing on transforming data into data models, once
data is extracted and loaded into a warehouse or data lake, there are several steps involved in building and updating the ML model.


##### The Emergence of ELT over ETL
ETL was the gold standard of data pipeline patterns for decades.
Though it’s still used, more recently ELT has emerged as the pattern of choice. 
###### Why? Prior to the modern breed of data warehouses,
Initially data teams didn’t have access to data warehouses with the storage or compute necessary to
handle loading vast amounts of raw data and transforming it into usable data models all in the same place.
In addition, data warehouses at the time were row-based databases that worked well
for transactional use cases, but not for the high-volume, bulk queries that are commonplace in analytics. Thus, data was first extracted
from source systems and then transformed on a separate system before being loaded into a warehouse for any final data modeling
and querying by analysts and visualization tools.

The majority of today’s data warehouses are built on highly scalable columnar databases that can both store and run bulk transforms on
large datasets in a cost-effective manner. Thanks to the I/O efficiency of a columnar database, data compression, and the ability
to distribute data and queries across many nodes that can work together to process data, things have changed. 
It’s now better to focus on extracting data and loading it into a data warehouse where
you can then perform the necessary transformations to complete the pipeline.

###### The impact of the difference between row-based and column-based data warehouses cannot be overstated. 
### Relational DB
- A relational database is ideal for transactional ( Online Transaction Processing (OLTP) )applications because it stores rows of data.
- Incremental data loading
- Queries against only a few rows


### Columnar DB
- A columnar database is preferred for analytical applications because it allows for fast retrieval of columns of data. Columnar databases are designed for data warehousing and big data processing because they scale using distributed clusters of low-cost hardware to increase throughput.
- Columnar databases are column based. They are built for speed because when data is stored by column, you can skip non-relevant data and immediately read what you are looking for. This makes aggregation queries especially fast.
- Columnar databases excel at Queries that involve only a few columns
- Great for Aggregation queries against vast amounts of data
- Column-wise compression

In the image below, just imagine an ecommerce application that has a table that records customers details and order details of what they purchase, logically to query this data, you have to merge both row tables ( relational together) to get information for what analysis you intend to run, Columnar is much faster because each block contains data of the same type, making compression optimal.
![columnar and row!](/images/columnar.png "columnar")



### HANDS ON EXPERIENCE
```shell
python3 -m venv env
source env/bin/activate
pip install -r requirements.txt
touch secrets.conf
```
secrets.conf contains all your secrets eg, aws acess keys, s3 bucket ARN , please do not commit it to version control
Create an AWS Account and do with following below with terraform 

- Create IAM role and select use case  Redshift - Customizable with Permissions (`AmazonS3FullAccess`) and keep Access key ID and Secret access key
- Create an IAM user (attached with IAM role created above) to access your Redshift cluster.
- Create a VPC group
- Create security group roles with inbound rules.
- Launch a Redshift Cluster with compute resources such as memory, ec2 instances
- Create an S3 bucket and upload your log data
- Create a PostgreSQL DB Instance using RDS and apply resource configuration setting
- Destroy all resources when no longer in use

### Provisioning Infrastructure
Policies are JSON documents that define explicit allow/deny privileges to specific resources or resource groups.

There are advantages to managing IAM policies in Terraform rather than manually in AWS. With Terraform, you can reuse your policy templates and ensure the principle of least privilege with resource interpolation.
In the step below, I will create an IAM user and an S3 bucket. Then, I will map permissions for that bucket with an IAM policy. 
In upcoming steps I will attach add more complex policies. The files responsible for infrastructure provisioning are in `Iac_terraform`folders
```hcl
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 3.0"
    }
  }
}

provider "aws" {
  region = var.region
}


resource "aws_iam_user" "new_user" {
  name = var.iam_user
}

resource "aws_s3_bucket" "bucket" {
  bucket = var.bucket_name
  acl    = "private"

  tags = {
    Name        = "My bucket"
    Environment = "Dev"
  }
}

data "aws_iam_policy_document" "example" {
  statement {
    actions   = ["s3:ListAllMyBuckets"]
    resources = ["arn:aws:s3:::*"]
  }
  statement {
    actions   = ["s3:*"]
    resources = [aws_s3_bucket.bucket.arn]
  }
}


resource "aws_iam_policy" "policy" {
  name        = "${random_pet.pet_name.id}_policy"
  description = "My test policy for datawarehouse in cloud"

  policy = data.aws_iam_policy_document.example.json

}

resource "aws_iam_user_policy_attachment" "attachment" {
  user       = aws_iam_user.new_user.name
  policy_arn = aws_iam_policy.policy.arn
}

```
```shell
terraform -version
```
Output:
```shell
Terraform v0.13.5

Your version of Terraform is out of date! The latest version
is 1.0.4. You can update by downloading from https://www.terraform.io/downloads.html
```
Export your access keys 
```shell
$ export AWS_ACCESS_KEY_ID="AK***************"
$ export AWS_SECRET_ACCESS_KEY="con***********************"
$ terraform init
$ terraform plan
$ terraform plan
```

Output:
```shell
<= data "aws_iam_policy_document" "example"  {
      + id   = (known after apply)
      + json = (known after apply)

      + statement {
          + actions   = [
              + "s3:ListAllMyBuckets",
            ]
          + resources = [
              + "arn:aws:s3:::*",
            ]
        }
      + statement {
          + actions   = [
              + "s3:*",
            ]
          + resources = [
              + (known after apply),
            ]
        }
    }

  # aws_iam_policy.policy will be created
  + resource "aws_iam_policy" "policy" {
      + arn         = (known after apply)
      + description = "My test policy for datawarehouse in cloud"
      + id          = (known after apply)
      + name        = "s3Policy"
      + path        = "/"
      + policy      = (known after apply)
      + policy_id   = (known after apply)
      + tags_all    = (known after apply)
    }

  # aws_iam_user.new_user will be created
  + resource "aws_iam_user" "new_user" {
      + arn           = (known after apply)
      + force_destroy = false
      + id            = (known after apply)
      + name          = "oluchipractise"
      + path          = "/"
      + tags_all      = (known after apply)
      + unique_id     = (known after apply)
    }

  # aws_iam_user_policy_attachment.attachment will be created
  + resource "aws_iam_user_policy_attachment" "attachment" {
      + id         = (known after apply)
      + policy_arn = (known after apply)
      + user       = "oluchipractise"
    }

  # aws_s3_bucket.bucket will be created
  + resource "aws_s3_bucket" "bucket" {
      + acceleration_status         = (known after apply)
      + acl                         = "private"
      + arn                         = (known after apply)
      + bucket                      = "oluchi-bucket-practise"
      + bucket_domain_name          = (known after apply)
      + bucket_regional_domain_name = (known after apply)
      + force_destroy               = false
      + hosted_zone_id              = (known after apply)
      + id                          = (known after apply)
      + region                      = (known after apply)
      + request_payer               = (known after apply)
      + tags                        = {
          + "Environment" = "Dev"
          + "Name"        = "My bucket"
        }
      + tags_all                    = {
          + "Environment" = "Dev"
          + "Name"        = "My bucket"
        }
      + website_domain              = (known after apply)
      + website_endpoint            = (known after apply)

      + versioning {
          + enabled    = (known after apply)
          + mfa_delete = (known after apply)
        }
    }

Plan: 4 to add, 0 to change, 0 to destroy.

------------------------------------------------------------------------

Note: You didn't specify an "-out" parameter to save this plan, so Terraform
can't guarantee that exactly these actions will be performed if
"terraform apply" is subsequently run.
```

```shell
pply complete! Resources: 3 added, 0 changed, 0 destroyed.

Outputs:

rendered_policy = {
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "",
      "Effect": "Allow",
      "Action": "s3:ListAllMyBuckets",
      "Resource": "arn:aws:s3:::*"
    },
    {
      "Sid": "",
      "Effect": "Allow",
      "Action": "s3:*",
      "Resource": "arn:aws:s3:::oluchi-bucket-practise"
    }
  ]
}
```
Visit your AWS console ( web service), click on the iam user and download the security credentials
![AWS console!](/images/aws_console_iam.png "AWS console")



Boto3 is the AWS SDK for Python would be installed with pip.

##### Sources
1. Data Pipelines Pocket Reference Book by James Densmore




